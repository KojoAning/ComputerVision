{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qqqU wandb transformers lightning albumentations torchmetrics torchinfo\n",
    "# %pip install -qqq requests gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install opencv-python\n",
    "# %pip install numpy\n",
    "# %pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the neccessary python libraries\n",
    "\n",
    "In this project there were some various libraries which need to be imported to be able to allow us achieve our desired task\n",
    "these libraries include \n",
    "* `os` -- \n",
    "* `zipfile`--\n",
    "* `platform`--\n",
    "* `warnings`--\n",
    "* `glob`--\n",
    "* `dataclass` -- \n",
    "* `wandb` -- \n",
    "* `opencv-python`-- (imported as cv)  this library helps us view and manipulate image data\n",
    "* `requests`  -- this library helps us download our dataset from the internet\n",
    "* `numpy` --\n",
    "* \n",
    "* `matplotlib` -- this library helps us plot our image data \n",
    "* `torch` -- this is the library used for the training of the dataset\n",
    "* `Albumentations` -- for augmentation of the image data\n",
    "* `Transformers` --  To load the transformer model\n",
    "* `Pytorch lightning module` -- To simplify and structure code implementations\n",
    "* `torchmetrics` -- For evaluating the models performance\n",
    "* `torchinfo` -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import platform\n",
    "import warnings\n",
    "from glob import glob\n",
    "from dataclasses import dataclass\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb=8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2 as cv \n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "from torchmetrics import MeanMetric\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DatasetConfig:\n",
    "    NUM_CLASSES: int = 4\n",
    "    IMAGE_SIZE: tuple[int,int] = (288, 288)\n",
    "    MEAN:tuple = (0.485, 0.456, 0.406)\n",
    "    STD: tuple = (0.229, 0.224, 0.225)\n",
    "    URL: str = r\"https://www.dropbox.com/scl/fi/r0685arupp33sy31qhros/dataset_UWM_GI_Tract_train_valid.zip?rlkey=w4ga9ysfiuz8vqbbywk0rdnjw&dl=1\"\n",
    "    # DATASET_PATH: str = os.path.join(os.getcwd(),'dataset_UWM_GI_Tract_train_valid')\n",
    "    DATASET_PATH: str = r\"C:\\Users\\ANING\\OneDrive\\Documents\\python\\dataset_UWM_GI_Tract_train_valid\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSegmentation(Dataset):\n",
    "    def __init__(self,is_training,mean,std,img_size,images,masks):\n",
    "        self.img_size = img_size\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.is_training = is_training\n",
    "        self.transforms = self.transformations(self.mean,self.std)\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def load_images(self,image,depth= 0):\n",
    "        image = cv.imread(image,depth)\n",
    "        if depth == cv.IMREAD_COLOR:\n",
    "            cv.cvtColor(image,cv.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            pass\n",
    "        return cv.resize(image,self.img_size,interpolation=cv.INTER_NEAREST)\n",
    "\n",
    "    def transformations(self,mean,std):\n",
    "        transformers = []\n",
    "\n",
    "        if self.is_training ==True:\n",
    "            transformers.extend([\n",
    "                A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5),\n",
    "                A.ShiftScaleRotate(scale_limit=0.12, rotate_limit=0.15, shift_limit=0.12, p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.5),\n",
    "                A.CoarseDropout(max_holes=8, max_height=self.img_size[1]//20, max_width=self.img_size[0]//20, min_holes=5, fill_value=0, mask_fill_value=0, p=0.5)\n",
    "            ])\n",
    "        \n",
    "        transformers.extend([\n",
    "                A.Normalize(mean=mean, std=std, always_apply=True),\n",
    "                ToTensorV2(always_apply=True),  # (H, W, C) --> (C, H, W)\n",
    "            ])\n",
    "\n",
    "        return A.Compose(transformers)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.load_images(self.images[index],depth=cv.IMREAD_COLOR)\n",
    "        mask = self.load_images(self.masks[index],depth=cv.IMREAD_GRAYSCALE)\n",
    "        transformed = self.transforms(image = image,mask = mask)\n",
    "        image,mask = transformed['image'] , transformed['mask'].to(torch.long)\n",
    "        return image,mask\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SegmentationDataset(pl.LightningDataModule):\n",
    "    def __init__(self,num_classes=7,img_size=(288, 288),ds_mean=(0.485, 0.456, 0.406),ds_std=(0.229, 0.224, 0.225),batch_size=32,num_workers=12, pin_memory=False,shuffle_validation=False,) :\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.mean = ds_mean\n",
    "        self.batch_size = batch_size\n",
    "        self.std  = ds_std\n",
    "        self.img_size = img_size\n",
    "        self.shuffle_validation = shuffle_validation\n",
    "        self.pin_memory = pin_memory\n",
    "        self.num_workers = num_workers\n",
    "      \n",
    "\n",
    "    def setup(self,*args,**kwargs):\n",
    "       train_images = sorted(glob(os.path.join(DatasetConfig.DATASET_PATH,'train','images',r'*.png')))\n",
    "       train_masks = sorted(glob(os.path.join(DatasetConfig.DATASET_PATH,'train','masks',r'*.png')))\n",
    "       valid_images = sorted(glob(os.path.join(DatasetConfig.DATASET_PATH,'valid','images',r'*.png')))\n",
    "       valid_masks = sorted(glob(os.path.join(DatasetConfig.DATASET_PATH,'valid','masks',r'*.png')))\n",
    "       \n",
    "       self.train_ds = DatasetSegmentation(is_training=True,mean=self.mean,std=self.std,images=train_images,masks=train_masks,img_size=self.img_size)\n",
    "       self.valid_ds = DatasetSegmentation(is_training=False,mean=self.mean,std=self.std,images=valid_images,masks=valid_masks,img_size=self.img_size)\n",
    "\n",
    "       \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds,batch_size=self.batch_size,drop_last=True,num_workers=self.num_workers,pin_memory=self.pin_memory,shuffle=True,)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return  DataLoader(self.valid_ds,batch_size=self.batch_size,drop_last=False,num_workers=self.num_workers,shuffle=self.shuffle_validation)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SegmentationDataset(\n",
    "    num_classes=DatasetConfig.NUM_CLASSES,\n",
    "    img_size=DatasetConfig.IMAGE_SIZE,\n",
    "    ds_mean=DatasetConfig.MEAN,\n",
    "    ds_std=DatasetConfig.STD,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    shuffle_validation=True,\n",
    ")\n",
    "\n",
    "sd.setup()\n",
    "\n",
    "train_batch = sd.train_dataloader()\n",
    "valid_batch = sd.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images,masks in train_batch:\n",
    "    print(masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_color = {\n",
    "    0 : (0,0,0),\n",
    "    1 : (0,0,255),\n",
    "    2 : (0,255,0),\n",
    "    3 : (255,0,0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_label(msks,color_map):\n",
    "    mask = np.zeros(msks.shape[:2] + (3,))\n",
    "    img_mask = np.squeeze(msks)\n",
    "\n",
    "    for k in color_map.keys():\n",
    "        mask[k == img_mask] = color_map[k]\n",
    "\n",
    "    return np.float32(mask)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_mask(image,mask):\n",
    "    alpha = 1.0\n",
    "    beta = 0.7\n",
    "    gamma = 0.0\n",
    "\n",
    "    mask = cv.cvtColor(mask,cv.COLOR_RGB2BGR)\n",
    "    image = cv.cvtColor(image,cv.COLOR_RGB2BGR)\n",
    "\n",
    "    image = cv.addWeighted(image,alpha,mask,beta,gamma)\n",
    "    image = cv.cvtColor(image,cv.COLOR_BGR2RGB)\n",
    "\n",
    "    return np.clip(image,0.0,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_and_mask(*, images, masks, color_map=index_to_color):\n",
    "    title = [\"GT Image\", \"Color Mask\", \"Overlayed Mask\"]\n",
    "\n",
    "    for idx in range(images.shape[0]):\n",
    "        image = images[idx]\n",
    "        grayscale_gt_mask = masks[idx]\n",
    "\n",
    "        fig = plt.figure(figsize=(15, 4))\n",
    "\n",
    "        # Create RGB segmentation map from grayscale segmentation map.\n",
    "        rgb_gt_mask = mask_to_label(grayscale_gt_mask, color_map=color_map)\n",
    "\n",
    "        # Create the overlayed image.\n",
    "        overlayed_image = overlay_mask(image, rgb_gt_mask)\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(title[0])\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(title[1])\n",
    "        plt.imshow(rgb_gt_mask)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.imshow(rgb_gt_mask)\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(title[2])\n",
    "        plt.imshow(overlayed_image)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensors, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    for c in range(3):\n",
    "        tensors[:, c, :, :].mul_(std[c]).add_(mean[c])\n",
    "\n",
    "    return torch.clamp(tensors, min=0.0, max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, masks in valid_batch:\n",
    "\n",
    "    images = denormalize(images, mean=DatasetConfig.MEAN, std=DatasetConfig.STD).permute(0, 2, 3, 1).numpy()\n",
    "    masks  = masks.numpy()\n",
    "    \n",
    "    display_image_and_mask(images=images, masks=masks)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name,num_classes):\n",
    "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_classes,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SegmentationModel(pl.LightningModule):\n",
    "    def __init__(self, model_name,num_classes):\n",
    "        super().__init__()\n",
    "        self.model = get_model(model_name,num_classes)\n",
    "\n",
    "\n",
    "    def forward(self,data):\n",
    "        output = self.model(pixel_values =data,return_dict=True)\n",
    "        unsampled_logists = F.interpolate(output['logists'],size=data.shape[-2:],mode='bilinear',align_corners=False)\n",
    "        return unsampled_logists\n",
    "\n",
    "    def training_step(self, batch, *args,**kwargs):\n",
    "        data, target = batch\n",
    "        output = self(data)\n",
    "        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, *args, **kwargs):\n",
    "        data, target = batch\n",
    "        output = self(data)\n",
    "        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.log(\"epoch\", self.current_epoch)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "    BATCH_SIZE:      int = 48 # 8\n",
    "    NUM_EPOCHS:      int = 100\n",
    "    INIT_LR:       float = 3e-4\n",
    "    NUM_WORKERS:     int = 0 if platform.system() == \"Windows\" else os.cpu_count()\n",
    "\n",
    "    OPTIMIZER_NAME:  str = \"AdamW\"\n",
    "    WEIGHT_DECAY:  float = 1e-4\n",
    "    USE_SCHEDULER:  bool = True # Use learning rate scheduler?\n",
    "    SCHEDULER:       str = \"MultiStepLR\" # Name of the scheduler to use.\n",
    "    MODEL_NAME:str = \"nvidia/segformer-b4-finetuned-ade-512-512\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegmentationModel(\n",
    "    model_name=TrainingConfig.MODEL_NAME,\n",
    "    num_classes=DatasetConfig.NUM_CLASSES,\n",
    ")\n",
    "\n",
    "\n",
    "data_module = SegmentationDataset(\n",
    "    num_classes=DatasetConfig.NUM_CLASSES,\n",
    "    img_size=DatasetConfig.IMAGE_SIZE,\n",
    "    ds_mean=DatasetConfig.MEAN,\n",
    "    ds_std=DatasetConfig.STD,\n",
    "    batch_size=TrainingConfig.BATCH_SIZE,\n",
    "    num_workers=TrainingConfig.NUM_WORKERS,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(\n",
    "    monitor=\"valid/f1\",\n",
    "    mode=\"max\",\n",
    "    filename=\"checkpoint{epoch:03d}-vloss_{valid/loss:.4f}_vf1_{valid/f1:.4f}\",\n",
    "    auto_insert_metric_name=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\", \n",
    "    devices=\"auto\",  \n",
    "    strategy=\"auto\",  \n",
    "    max_epochs=TrainingConfig.NUM_EPOCHS,  \n",
    "    enable_model_summary=True,  \n",
    "    precision=\"16-mixed\",  \n",
    "    callbacks=[model_checkpoint]\n",
    " \n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "dataset_path = os.path.join(os.getcwd(),'dataset_UWM_GI_Tract_train_valid.zip')\n",
    "dataset_path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ['l']\n",
    "p.extend(['p','o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l', 'p', 'o']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
